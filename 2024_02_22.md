# 2024_02_22

python scripts/train_expressive.py --config=config/pose_diffusion_expressive_French_both.yml



**In_text:**  is this same to the **word_indices:** ?

tensor([[3929,    0,  262,    0,    0,    0,    0, 5003,    0,    0,    0,   78,
            0,    0,    0,    0, 5004,    0,  648,    0,    0,  997,    0,    0,
         1429,    0,    0,    0,   33,    0,    0,  648,    0,    0]],
       device='cuda:0')





```
def extend_word_seq(lang, words, end_time=None):
    n_frames = self.n_poses
    if end_time is None:
        end_time = aux_info['end_time']
    frame_duration = (end_time - aux_info['start_time']) / n_frames

    extended_word_indices = np.zeros(n_frames)  # zero is the index of padding token
    if self.remove_word_timing:
        n_words = 0
        for word in words:
            idx = max(0, int(np.floor((word[1] - aux_info['start_time']) / frame_duration)))
            if idx < n_frames:
                n_words += 1
        space = int(n_frames / (n_words + 1))
        for i in range(n_words):
            idx = (i+1) * space
            extended_word_indices[idx] = lang.get_word_index(words[i][0])
    else:
        prev_idx = 0
        for word in words:
            idx = max(0, int(np.floor((word[1] - aux_info['start_time']) / frame_duration)))
            if idx < n_frames:
                extended_word_indices[idx] = lang.get_word_index(word[0])
                # extended_word_indices[prev_idx:idx+1] = lang.get_word_index(word[0])
                prev_idx = idx
    return torch.Tensor(extended_word_indices).long()
```






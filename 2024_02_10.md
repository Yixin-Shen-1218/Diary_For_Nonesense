# 2024_02_10

1. The output size of the audio_feat is 32
1. torch.unsqueeze(n)在第n个维度拓展一个维度
1. Learn the Conv1D







python scripts/train_expressive.py --config=config/pose_diffusion_expressive_French.yml







I find I make a mistake that the DiffGesture model doesn't use the text script part as the model input, so last time the experiment is not rational?

1. The lang_model part of the saved checkpoints are all not right. Therefore, maybe I need to regenerate the **speaker_model** and test the HA2G part again since they use the text script.
   1. make the lang on DiffGesture part -> retrain feature_extractor?
2. The 



